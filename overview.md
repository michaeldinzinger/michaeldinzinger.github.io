---
layout: table
table_include: overview_table.html
table_sortlist: "{sortList: [[1,1]]}"
---

Overview
========

Since 2016, CommonCrawl regularly publishes the [robots.txt files](https://commoncrawl.org/blog/robotstxt-and-404-redirect-data-sets) that were fetched during the CCBot's web crawl. The robots.txt dumps are published along with the regular WARC, WAT and WET files in intervals of approximately two to three months. We have parsed the last robots.txt dumps of each year since 2016, resulting in eight years of collected statistics. The following table outlines each year together with the period, in which the documents were fetched, as well as the total number of fetches. As the dumps also contain unsuccessful fetches (e.g. HTTP status code 404) and unparsable files, the last column of the table shows the total number of successfully parsed robots.txt files.