---
layout: basic
table_content_length_all: tables/basic-content-length-all.html
table_content_length_curlie: tables/basic-content-length-curlie.html
table_num_lines_all: tables/basic-num-lines-all.html
table_num_lines_curlie: tables/basic-num-lines-curlie.html
table_num_user_agents_all: tables/basic-num-user-agents-all.html
table_num_user_agents_curlie: tables/basic-num-user-agents-curlie.html
table_crawldelay: tables/basic-crawldelay.html
table_num_sitemaps: tables/basic-num-sitemaps.html
---

Basic statistics
================

This page presents basic statistics on the collected robots.txt files and their development over the years. The statistics were calculate over all parsed files and additionally aggregated for 16 website categories. We therefore employed the [Curlie](https://curlie.org/) top level label to categorize the host of the respective URL (Example: https://**cnn.com**/robots.txt -> News). Note that the human-curated, filtered Curlie directory contains less than one million of hosts and consequently, most robots.txt files remain unlabeled.